Neural Architecture search (NAS) is a method to automate the design of [[Neural network]].

Essentially, it allows us to design high-performing deep learning model architectures using other ML models, billions of times faster than humans

It's been used to design networks that are on par or even outperform hand-designed architectures. 

The means for doing so can be categorized according to a search space, search strategy, and performance estimation strategy.

- The **search space** defines the type of [[Neural network]] that can be designed and optimized
- The **search strategy** defines the approach used to explore the search space
- The **performance estimation strategy** assesses the performance of a [[Neural network]] from its design (w/o constructing and training it).
