#mathematics 
Gradient descent is an optimization algorithm commonly used in neural networks that involves [[Forward propagation]] to get the prediction of a model, [[Backpropagation]] to get the gradients of the loss with respect to the params of a model, and then an [[Update rule]] to optimize the params of a model through a [[learning rate]] for a specific number of epochs to the global minima of a [[Loss function]].

	![[Screenshot 2024-06-08 at 8.36.16 AM.png | 400]]


For the mathematics, check out the [[Mathematics of Gradient Descent (NN)]]


